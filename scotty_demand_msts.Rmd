---
title: "Demand Forecasting of Scotty Ride-sharing Service"
author: "by Gabriel Erichson"
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: true
    number_sections: true
    theme: sandstone
    highlight: haddock
    css:  style.css
  pdf_document: default
---

Reference:
[Forecasting: Principles and Practice](https://otexts.com/fpp2/)<br>
[Exploring & Visualizing Time Series](https://afit-r.github.io/ts_exploration)<br>

<head>
		<title>"Demand Forecasting</title>
		<link rel="icon" href="assets/logo.png" type="image/png">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
</head>

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	fig.align = "center",
	message = FALSE,
	warning = FALSE,
	comment = "#>",
	result = "hide"
)

options(scipen = 9999999)
#rm(list=ls())

#Data Wrangling and Visualization
library(Rcpp)
library(tidyverse)
library(ggplot2)
library(plotly)
library(scales)
library(lubridate)
library(tidyquant)
library(glue)
library(ggthemes)
library(paletti)
library(kableExtra)
library(sqldf)
library(gridExtra)
library(grid)
library(recipes)


#Time Series 
library(padr)
library(TSstudio)
library(forecast)
library(tseries)
library(magrittr)

```


```{r, include=FALSE}
# WARNA
mycolorfill = c(
  
  light_blue="#2f4b7c", 
  smooth_blue ="#4B87CB",
  light_purple ="#665191",
  dark_pink="#a05195", 
  light_pink="#d45087", 
  light_red="#f95d6a", 
  dark_orange="#ff6347",
  semi_orange="#e79658",
  orange="#dda15a",
  cream="#b59378",
  dark_cream="#A57F5F",
  choc="#85664B",
  dark_choc="#6b5340",
  light_orange="#ff7c43"
)


viz_palette(mycolorfill)
mycolor_fill  <- get_scale_fill(get_pal(mycolorfill))
mycolor_color <- get_scale_color(get_pal(mycolorfill))
mycolor_hex <- get_hex(mycolorfill)

```


# Intro

Scotty Technologies Inc. ("Scotty") adalah sebuah perusahaan start-up teknologi yang didirikan pada tahun 2017 di Istanbul, Turkey. Salah satu layanan utama dari Scotty adalah *motorcycle ride-sharing* atau yang akrab kita ketahui *Ojek Online*.
Menurut informasi yang dilansir dari [markets.businessinsider.com](https://markets.businessinsider.com/news/stocks/bcap-invests-in-turkish-rideshare-company-scotty-1028819466), Scotty berencana untuk menjadi super-app pertama di Turkey dengan tekonologi dan model bisnis yang distruptif, serta berencana untuk melanjutkan pertumbuhan yang menjanjikan dan mencapai profitabilitas dalam waktu dekat. Berbicara mengenai pertumbuhan, tentu saja tidak luput dari demand. Oleh karena itu, projek ini ditujukan untuk membuat model analisa dan memprediksi demand transaksi per-jam pada layanan motorcycle ride-sharing di Scotty. Dalam proses pembuatan model ini, kita menggunakan data transaksi Scotty dari 2017-10-01 sampai 2017-12-02. <br>


# Problem Identification

Problem yang diselesaikan yaitu membuat model untuk memprediksi demand transaksi per-jam pada layanan motorcycle ride-sharing di Scotty. Namun yang hendak diprediksi dalam rentang waktu berapa lama? Mari kita cek dulu supaya dapat membantu proses pemodelan yang akan kita buat.

```{r}
# Read Data Test
scotty_test <- read_csv("data_input/scotty_ts/data-test.csv")
glimpse(scotty_test)
```

Struktut data diatas belum sesuai. Mari sesuaikan struktur datanya dan meilhat rentang waktu yang hendak diprediksi dahulu:
```{r}
scotty_test <- scotty_test %>% 
   mutate(
      # Transaksi Per-Jam
      datetime = floor_date(datetime, unit="hour"),
      demand = as.integer(demand)
   )
summary(scotty_test)
```

Data diatas merupakan summary dari data yang hendak diprediksi. Berdasarkan informasi tersebut, dapat disimpullkan bawah masalah yang perlu kita selesaikan yaitu melakukan prediksi **demand per-jam** di sub area **sxk97, sxk9e dan sxk9** dalam rentang waktu dari **2017-12-03 00:00:00 sampai 2017-12-09 23:00:00** atau **24 jam** * **1 minggu**. Oke, mari kita mulai proses nya.


# Data Preparation
## Read Data
Berikut ini adalah data yang kita punya untuk melakukan analisa dan prediksi.

```{r}
scotty_input <- read_csv("data_input/scotty_ts/data-train.csv")
```

**5 Top Line Data**
```{r}
head(scotty_input)
```

<br>
**5 Bottom Line Data**
```{r}
tail(scotty_input,5)
```

***

<br>

## Variable Description

Variable              | Description
----------------------|------------------------------------------------------------
id                    | Transaction id
trip_id               | Trip id
driver_id             | Driver id
rider_id              | Rider id
start_time            | Transaction Time
src_lat               | Request source latitude
src_lon               | Request source longitude
src_area              | Request source area
src_sub_area          | Request source sub-area
dest_lat              | Requested destination latitude
dest_lon              | Requested destination longitude
dest_area             | Requested destination area
dest_sub_area         | Requested destination sub-area
distance              | Trip distance (in KM)
status                | Trip status (all status considered as a demand)
confirmed_time_sec    | Time different from request to confirmed (in seconds)

***

<br>

## Missing & Duplicate Value?
**Missing Value**
Jika dilihat terdapat missing value pada variabel **trip_id** dan **driver_id**, namun tidak menjadi masalah karena pada case ini kita butuh data waktu, sub area dan demand.
```{r}
colSums(is.na(scotty_input))
```
<br>

**Duplicate Value**
Dataset ini tidak memiliki data transaksi yang duplikat, sehingga bisa kita lanjutkan ke tahap pre-processing.
```{r}
data.frame(jumlah.observasi.awal=length(scotty_input$id),
           jumlah.observasi.unik=length(unique(scotty_input$id)))
```

***

<br>

# Data Pre-processing
## Data Structure
```{r}
glimpse(scotty_input)
```

Dataset yang dimiliki terdiri dari **90.133** observasi dan **16** variabel. Jika dilihat dari struktur datanya, beberapa variabel memiliki tipe data yang belum sesuai. Namun, karena projek ini bertujuan untuk melakukan time-series forecasting terhadap demand per-jam pada layanan motorcycle ride-sharing di Scotty, maka data yang kita perlukan adalah waktu transaksi per-jam dan demand pada setiap src_sub_area. Maka dari Berikut prosesnya:

```{r}
scotty_input <- scotty_input %>% 
   mutate(
      # Transaksi Per-Jam
      datetime = floor_date(start_time, unit="hour")
   ) %>% 
   group_by(src_sub_area, datetime) %>% 
   summarise(demand = length(status)) %>% 
   ungroup()
```


## Missing Sequential Datetime Value

Dalam melakukan analisa dan prediksi time series ada beberapa hal yang harus terpenuhi antara lain:<br>
  1. Data tidak boleh ada yang missing<br>
  2. Data harus terurut berdasarkan periode waktunya<br>
  3. Tidak boleh ada waktu atau periode yang terlewat/bolong<br>
  
Terkait case ini, Jam opearasional Scotty yaitu 24jam/hari, sehingga kita harus memastikan seluruh data jam sudah lengkap secara sekuensial dari rentang tanggal minimum hingga tanggal maximum. Jika terdapat deret waktu yang kosong, 
maka dapat dilakukan imputasi data dengan asumsi bahwa pada waktu tersebut memang tidak terdapat transaksi, sehingga **jumlah demand = 0**.

```{r, warning=FALSE, error=FALSE}
scotty_input <- scotty_input %>% 
  group_by(src_sub_area) %>% 
  # PAD tanggal sekuensial
  padr::pad(start_val = min(scotty_input$datetime), end_val = max(scotty_input$datetime)) %>% 
  ungroup() %>% 
  distinct() %>% 
  mutate(
    # replace NA Value menjadi 0
    demand = ifelse(is.na(demand),0,demand)
  ) %>% 
  arrange(datetime)
```

**Berikut data hari yang tidak memiliki transaksi:**
```{r}
scotty_input %>% filter(demand==0)
```

Data diatas menunjukan total waktu yang tidak terdapat transaksi dari sub Area **sxk97, sxk9e dan sxk9s** yaitu 301 Jam.



## Data Summary
```{r}
summary(scotty_input)
```

Informasi di atas merupakan summary keseluruhan data yang kita miliki. Dapat diketahui bahwa dataset ini merupakan data transaksi dari 3 sub area yaitu **sxk97, sxk9e dan sxk9s** yang terjadi dari **2017-10-01 00:00** sampai **2017-12-02 23:00** dengan **jumlah minimum 0 demand** dan **jumlah maximum 217 demand**. Demand 0 diasumsikan bahwa pada waktu tertentu memang tidak terdapat transaksi.


***

<br>

# Exploratory Data Analysis

## Demand Analysis by Sub-Area
Berikut ini visualisasi demand berdasarkan sub-area dari dari **2017-10-01 00:00** sampai **2017-12-02 23:00**. Titik/Poin berwarna merah, hijau dan biru menandakan data deret waktu tersebut kosong atau tidak terdapat transaksi pada waktu tersebut.

```{r, fig.width=8.5, fig.asp=0.75}
scotty_demand <- scotty_input 

scotty_demand <- scotty_demand %>% 
    mutate(
      weekdays = weekdays(datetime),
      popup = glue(
      "Category: {src_sub_area}
      Transaction Time: {datetime}
      Day: {weekdays}
      Demand: {demand}"
    )) 


plot_scotty_demand <- ggplot(scotty_demand,aes(x=datetime, y=demand),show.legend = FALSE)+
   #geom_point()+
  geom_line(show.legend = FALSE,size=0.3)+
  geom_point(aes(text=popup),size=0.05)+
  geom_point(data = scotty_demand %>% filter(demand==0),
             aes(x=datetime, y=demand, color=src_sub_area, text=popup),size=0.5, show.legend = FALSE)+
  labs(
    x = NULL, 
    y = NULL,
    title = "Demand by Sub-Area"
  ) +
  scale_x_datetime(date_breaks = "7 day", labels = date_format("%b %d"))+
  facet_wrap(~ src_sub_area, scale = "free", ncol = 1)+
  theme_minimal()+
  theme(
    legend.position = "none", 
    plot.title = element_text(hjust = 0.5),
    panel.spacing = unit(1, "lines"))

ggplotly(p = plot_scotty_demand, tooltip="text") %>% 
  layout(showlegend=FALSE)
```

Pada grafik diatas dapat dilihat bahwa jumlah demand dari ketiga sub-area relatif sama dan juga terdapat **pola seasonal** meskipun belum terlihat jelas detailnya. Selain itu, juga dapat dilihat demand pada **Jumat,03 November 2017 pukul 18.00** dan **Jumat, 24 November 2017 pukul 18.00** jauh lebih tinggi dari hari lainnya. Sayangnya data yang kita miliki belum dapat menjawab hal tersebut, namun mari kita fokus ke demand. Berikut adalah demand per-harinya: 

```{r,fig.width=12.5}
# Daily Demand
polar_day <- scotty_demand %>% 
  mutate(day=weekdays(datetime)) %>% 
  group_by(src_sub_area,day) %>% 
  summarise(demand_per_day=sum(demand)) %>% 
  ungroup() %>% 
  mutate(
    day = ordered(day, levels=c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"))
  )

serror <- function(x) sqrt(var(x)/length(x)) 

ggplot(polar_day %>% filter(src_sub_area=="sxk97"), aes(x=day, y=demand_per_day, fill = day)) +
  geom_bar(width = 1, stat = "identity", color = "white", show.legend = FALSE) +
  geom_errorbar(aes(ymin = demand_per_day - serror(demand_per_day), 
                    ymax = demand_per_day + serror(demand_per_day), 
                    color = day), 
                    width = .2) + 
  scale_y_continuous(breaks = 0:nlevels(day)) +
  labs(
    title = "Daily Demand in Sub-Area sxk97"
  )+
 theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size=12, face="bold"),
        axis.text.y = element_blank(),
        axis.text.x=element_text(face="bold"))+
  coord_polar() -> day_polar1

ggplot(polar_day %>% filter(src_sub_area=="sxk9e"), aes(x=day, y=demand_per_day, fill = day)) +
  geom_bar(width = 1, stat = "identity", color = "white", show.legend = FALSE) +
  geom_errorbar(aes(ymin = demand_per_day - serror(demand_per_day), 
                    ymax = demand_per_day + serror(demand_per_day), 
                    color = day), 
                    width = .2) + 
  scale_y_continuous(breaks = 0:nlevels(day)) +
  labs(
    title = "Daily Demand in Sub-Area sxk9e"
  )+
  theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
         plot.title = element_text(hjust = 0.5, size=12, face="bold"),
        axis.text.y = element_blank(),
        axis.text.x=element_text(face="bold"))+
  coord_polar() ->day_polar2


ggplot(polar_day %>% filter(src_sub_area=="sxk9s"), aes(x=day, y=demand_per_day, fill = day)) +
  geom_bar(width = 1, stat = "identity", color = "white", show.legend = FALSE) +
  geom_errorbar(aes(ymin = demand_per_day - serror(demand_per_day), 
                    ymax = demand_per_day + serror(demand_per_day), 
                    color = day), 
                    width = .2) + 
  scale_y_continuous(breaks = 0:nlevels(day)) +
  labs(
    title = "Daily Demand in Sub-Area sxk9s"
  )+
 theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,size=12, face="bold"),
        axis.text.y = element_blank(),
        axis.text.x=element_text(face="bold"))+
  coord_polar() ->day_polar3


grid.arrange(day_polar1,day_polar2,day_polar3, ncol = 3)

```


Grafik diatas memberikan informasi total demmand per-hari dari masing-masing Sub-Area. Secara keseluruhan, ketiga Sub-Area menunjukan pola demand per-hari yang sama, dimana **total demand paling besar yaitu pada hari Jumat** dan **total demand paling kecil yaitu pada hari Minggu**. Mari kita lihat data demand per-jamnya berikut:

```{r, fig.width=12}
# Hourly Demand
polar_hourly <- scotty_demand %>% 
  mutate(hour=hour(datetime)) %>% 
  group_by(src_sub_area,hour) %>% 
  summarise(demand_per_hour=sum(demand)) %>% 
  ungroup() %>% 
  mutate(
    hour = ordered(hour,levels=c("0","1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17",
                              "18","19","20","21","22","23"))
  )

ggplot(polar_hourly %>% filter(src_sub_area=="sxk97"), aes(x=hour, y=demand_per_hour, fill = hour)) +
  geom_bar(width = 1, stat = "identity", color = "white", show.legend = FALSE) +
  geom_errorbar(aes(ymin = demand_per_hour - serror(demand_per_hour), 
                    ymax = demand_per_hour + serror(demand_per_hour), 
                    color = hour), 
                    width = .2) + 
  scale_y_continuous(breaks = 0:nlevels(hour)) +
  labs(
    title = "Hourly Demand in Sub-Area sxk97",
    subtitle = "24 Hour Format"
  )+
 theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,size=12, face="bold"),
        plot.subtitle = element_text(hjust = 0.5,size=11),
        axis.text.y = element_blank(),
        axis.text.x=element_text(size=11, face="bold"))+
  coord_polar() -> hour_polar1


ggplot(polar_hourly %>% filter(src_sub_area=="sxk9e"), aes(x=hour, y=demand_per_hour, fill = hour)) +
  geom_bar(width = 1, stat = "identity", color = "white", show.legend = FALSE) +
  geom_errorbar(aes(ymin = demand_per_hour - serror(demand_per_hour), 
                    ymax = demand_per_hour + serror(demand_per_hour), 
                    color = hour), 
                    width = .2) + 
  scale_y_continuous(breaks = 0:nlevels(hour)) +
  labs(
    title = "Hourly Demand in Sub-Area sxk9e",
    subtitle = "24 Hour Format"
  )+
 theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,size=12, face="bold"),
        plot.subtitle = element_text(hjust = 0.5,size=11),
        axis.text.y = element_blank(),
        axis.text.x=element_text(size=11, face="bold"))+
  coord_polar() -> hour_polar2


ggplot(polar_hourly %>% filter(src_sub_area=="sxk9s"), aes(x=hour, y=demand_per_hour, fill = hour)) +
  geom_bar(width = 1, stat = "identity", color = "white", show.legend = FALSE) +
  geom_errorbar(aes(ymin = demand_per_hour - serror(demand_per_hour), 
                    ymax = demand_per_hour + serror(demand_per_hour), 
                    color = hour), 
                    width = .2) + 
  scale_y_continuous(breaks = 0:nlevels(hour)) +
  labs(
    title = "Hourly Demand in Sub-Area sxk9s",
    subtitle = "24 Hour Format"
  )+
 theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,size=12, face="bold"),
        plot.subtitle = element_text(hjust = 0.5,size=11),
        axis.text.y = element_blank(),
        axis.text.x=element_text(size=11, face="bold"))+
  coord_polar() -> hour_polar3


grid.arrange(hour_polar1,hour_polar2,hour_polar3, ncol = 3)

```

Grafik diatas memberikan informasi total demmand per-jam dari masing-masing Sub-Area. Secara keseluruhan, ketiga Sub-Area menunjukan pola demand per-jam yang sama, dimana **total demand relatif tinggi dari pukul 17:00 hingga pukul 19:00** dan **total demmand relatif rendah dari pukul 01:00 hingga pukul 05:00**. Namun, yang berbeda adalah **total demand sub-area sxk9e juga cukup tinggi pada pukul 08:00**, hal ini berbeda dengan sub-area lainnya. 


***

<br>

## Seasonal Analysis

Sebelum melakukan forecasting kita perlu mengetahui informasi tren dan seasonal yang ada pada data. Salah satu cara untuk mengetahuinya yaitu melakukan Decompose. Decompose merupakan tahapan dalam analisis time series yang digunakan untuk menguraikan beberapa komponen yang berupa:<br>

  1. *Data* : data objek time-series yang diamati.<br>
  2. *Trend* : pola data secara general, cenderung untuk naik atau turun.<br>
  3. *Seasonal* : pola musiman yang membentuk pola berulang pada periode waktu yang tetap.<br>
  4. *Residual/Error* : pola yang tidak dapat ditangkap dalam trend dan seasonal.<br>
  
Jika pada hasil decompose, trend masih membentuk sebuah pola maka dapat dicurigai masih ada seasonality yang belum ditangkap oleh frekuensi data. Seharusnya trend cenderung naik atau cendurung turun atau membentuk garis yang smooth. 

Sesuai dengan masalah yang hendak diselesaikan yaitu melakukan prediksi demand per-jam pada masing-masing sub-area selama 1 minggu, maka mari kita coba decompose secara harian dan mingguan terhadap salah satu sub-area.


```{r, fig.width=8}
sxk9s <- scotty_input %>% filter(src_sub_area == "sxk9s") %>% select(-src_sub_area)

sxk9s_ts_weekly <- sxk9s %>% .$demand %>% ts(frequency = 24*7)
  
plot_sxk9s_ts_weekly <- sxk9s_ts_weekly %>% 
  tail(24*7*4) %>% 
  decompose() %>% 
   autoplot()+ 
    theme_minimal()+
    theme(
          title = element_text(size=11),
          axis.title=element_text(size=9, face="bold"),
          axis.text.x=element_text(size=9),
          axis.text.y=element_text(size=9),
          legend.position = "none",
          panel.spacing = unit(4, "pt")
          )


ggplotly(plot_sxk9s_ts_weekly) %>% 
  layout(title = list(text = paste0('Decomposition on Weekly in Sub-Area sxk9s',
                                    '<br>',
                                    '<sup>',
                                    'Deompose single seasonal time-series object using frequency = 24*7 (Weekly)',
                                    '</sup>')))

```

Hasil decompose objek single sesonal time-series diatas menggunakan frekuensi Mingguan pada data 1 bulan terkahir. Beberapa informasi yang bisa kita dapat yaitu:<br>

  1. Panel *data* merupakan data observasi time-series scotty pada data 1 bulan terkahir. Data ini sebenarnya sama seperti yang ditampilkan pada poin 5.1 Demand Analysis, namun karena pola kurang terlihat pada poin 5.1 tersebut maka kita perkecil analisanya menggunakan data 1 bulan terakhir.<br>
  2. Panel *seasonal* merupakan komponen seasonal. Jika dilihat terdapat 4 bagian grafik yang memuncak paling tinggi dan yang lainnya rendah. Berhubung data yang kita time-series yang kita buat menggunakan frekuensi mingguan, maka hal ini dapat mengindikasikan bahwa pola seasonal yang terjadi sebenarnya secara harian dan mingguan.<br>
  3. Panel *trend* merupakan komponen trend. Pola trend yang ditampilkan masih belum smooth dan terkesan masih membentuk pola pada beberapa bagian, mungkin saja terdapat pola seasonal yang belum ditangkap. Hal ini masuk akal dengan yang ditampilkan pada panel seasonal, sehingga mengindikasikan bahwa data kita merupakan *multiple seasonal time series object*.<br> 
  4. Panel *Remainder/Error* merupakan komponen error yang menunjukan pola yang tidak dapat ditangkap dalam trend dan seasonal.<br>


Hasil decompose diatas mengindikasikan bahwa data time-series yang kita observasi memiliki pola *Multi-Seasonal*. Berdasarkan pola seasonalnya menunjukan bahwa data ini memiliki seasonal harian dan mingguan, sehingga mari kita mebuat data multiple seasonal time series dan melakukan decompose kembali.


```{r, fig.width=8}
sxk9s_msts_weekly <- sxk9s %>% .$demand %>%  
  msts(seasonal.periods = c(24, 24*7))

plot_sxk9s_msts_weekly <- sxk9s_msts_weekly %>% 
  tail(24*7*4) %>% 
   mstl() %>% 
   autoplot()+ 
   labs(
      title = "Decomposition on Daily and Weekly in Sub-Area sxk9s"
    )+
    theme_minimal()+
    theme(
          title = element_text(size=12),
          axis.title=element_text(size=9, face="bold"),
          axis.text.x=element_text(size=9),
          axis.text.y=element_text(size=9),
          legend.position = "none",
          panel.spacing = unit(4, "pt")
          ) 

ggplotly(plot_sxk9s_msts_weekly) %>% 
    layout(title = list(text = paste0('Decomposition on Daily & Weekly in Sub-Area sxk9s',
                                    '<br>',
                                    '<sup>',
                                    'Deompose multiple seasonal time-series object using seasonal.periods = c(24, 24*7) (Daily & Weekly)',
                                    '</sup>')))

```

Pada hasil decompose objek multiple seasonal time-series diatas, dapat dilihat pola tren terlihat lebih smooth dan tidak membentuk pola. Hal ini mengindikasikan bahwa kemungkinan besar seluruhan seasonal sudah ditangkap. Untuk lebih jelasnya, berikut ini visualisasi seasonal perjam-nya dalam harian dan mingguan pada data observasi kita:


```{r, fig.width=8}
sxk9s_msts_weekly_dc <- mstl(sxk9s_msts_weekly %>% tail(24*7*4))

plot_seasonal_msts <- as.data.frame(sxk9s_msts_weekly_dc) %>% 
  mutate(
    datetime = sxk9s %>% tail(24*7*4) %>% .$datetime
  ) %>% 
  mutate(
    day = wday(datetime, label = TRUE, abbr = FALSE),
    hour = as.factor(hour(datetime))
  ) %>% 
  group_by(day, hour) %>% 
  summarise(
    seasonal = sum(Seasonal24 + Seasonal168)
  ) %>% 
  ggplot(mapping = aes(x = hour, y = seasonal)) +
  geom_col(aes(fill = day)) +
  scale_fill_viridis_d(option = "plasma") +
  labs(
    title = "Seasonal Analysis based Daily and Weekly in Sub-Area sxk9s",
    fill = "Day of Week"
  ) + 
  theme_minimal() +
  theme(
        title = element_text(size=11),
        axis.title=element_text(size=9, face="bold"),
        axis.text.x=element_text(size=9),
        axis.text.y=element_text(size=9),
        legend.position = "right"
        ) 

ggplotly(plot_seasonal_msts)

```


# Cross Validation
Dalam case ini kita akan mencoba membuat beberapa model, tentunya kita akan membagi seluruh data yang kita punya menjadi data train dan data test. Problem yang diselesaikan yaitu memprediksi demand perjam dalam 1 minggu sehingga saya akan membagi data test menggunakan data 1 minggu terakhir dan sisanya sebagai data train. Berikut interval waktu di data train dan data test:

```{r}
scotty_data <- scotty_input

#data test 1 minggu
test_size <- 24*7

test_end <- max(scotty_data$datetime)
test_start <- test_end - hours(test_size) + hours(1)

train_end <- test_start - hours(1)
train_start <- min(scotty_data$datetime)

# get the interval of each samples
intrain <- interval(train_start, train_end)
intest <- interval(test_start, test_end)

data.frame("interval_data_train"=intrain,
              "interval_data_test"=intest)
```

**Berikut ini visualisasi pembagian data train dan data test:**

```{r, fig.width=8}
ggplotly(
 scotty_data %>%
  mutate(sample = case_when(
    datetime %within% intrain ~ "train",
    datetime %within% intest ~ "test"
  )) %>%
  drop_na() %>%
  mutate(
    sample = factor(sample, levels = c("train", "test"))
  ) %>%
  ggplot(aes(x = datetime, y = demand, colour = sample)) +
    geom_line() +
    labs(
      x = NULL, 
      y = NULL,
      title = "Demand by Sub-Area on Data Train Vs Data Test"
    ) +
    scale_x_datetime(date_breaks = "7 day", labels = date_format("%b %d"))+
    facet_wrap(~ src_sub_area, scale = "free", ncol = 1) +
    theme_minimal()+
    theme(
      legend.position = "bottom", 
      plot.title = element_text(hjust = 0.5),
      panel.spacing = unit(1, "lines"))+
    mycolor_color()
  ) %>% 
  layout(
    xaxis = list(showticklabels = FALSE),
    legend=list(orientation = "h",
                   y = -0.1, x = 0.4)
  )


```


# Data Scalling
Seperti yang kita lihat pada visualisasi sebelumnya, dimana jumlah demand di 3 Sub-Area berbeda-beda dan pada waktu-waktu tertentu memiliki selisih jauh berbeda. Hal ini dapat mengindikasikan outlier, sehingga perlu dilakukan scaling data train pada masing-masing Sub-Area sebelum memulai pemodelan supaya tidak sensitif pada data outlier. Berikut prosesnya:

**1. Convert data to wide format**
```{r}
# Spread data menjadi 3 Sub-Area 
scotty_data <- scotty_data %>%
  spread(src_sub_area, demand)

head(scotty_data)

```

**2. Scalling data**
```{r}
# recipes: square root, center, scale
rec <- recipe(~ ., filter(scotty_data, datetime %within% intrain)) %>%
  step_sqrt(all_numeric()) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  prep()

# preview the bake results
scotty_data <- bake(rec, scotty_data)
scotty_data
```

**3. Convert datato long format**
```{r}
scotty_data <- scotty_data %>% 
  gather(src_sub_area, demand, -datetime)
head(scotty_data)
```


Data yang sudah di scalling tentunya perlu di kembalikan lagi seperi semula, maka dibuat fungsi:

```{r}
reverse <- function(vector, recipe, varname) {
  # store recipe values
  rec_center <- recipe$steps[[2]]$means[varname]
  rec_scale <- recipe$steps[[3]]$sds[varname]
  
  # convert back based on the recipe
  results <- (vector * rec_scale + rec_center) ^ 2
  
  # add additional adjustment if necessary
  results <- round(results)
  
  # return the results
  results
}
```


# NESTED MODEL FITTING AND FORECASTING

```{r}
scotty_data %<>%
  mutate(sample = case_when(
    datetime %within% intrain ~ "train",
    datetime %within% intest ~ "test"
  )) %>%
  drop_na()
  
scotty_data
```


```{r}
scotty_data <- scotty_data %>%
  group_by(src_sub_area, sample) %>%
  nest(.key = "data") %>%
  spread(sample, data)

scotty_data
```


# PREPARING THE DATA MODEL LIST

```{r}
# Membuat list model data
data_funs <- list(
  ts = function(x) ts(x$demand, frequency = 24*7),
  msts = function(x) msts(x$demand, seasonal.periods = c(24, 24 * 7))
)

data_funs
```


```{r}
# convert to nested
data_funs <- data_funs %>%
  rep(length(unique(scotty_data$src_sub_area))) %>%
  enframe("data_fun_name", "data_fun") %>%
  mutate(src_sub_area =
    sort(rep(unique(scotty_data$src_sub_area), length(unique(.$data_fun_name))))
  )

data_funs
```

```{r}
scotty_data <- scotty_data %>%
  left_join(data_funs)
```


# PREPARING THE TIME SERIES MODEL LIST
```{r}
# models list
models <- list(
  ets = function(x) ets(x),
   holt.winter = function(x) HoltWinters(x,seasonal = "additive"),
  stlm = function(x) stlm(x),
  auto.arima = function(x) auto.arima(x),
  tbats = function(x) tbats(x, use.box.cox = FALSE, 
                  use.trend = TRUE, 
                  use.damped.trend = TRUE,
                  use.parallel = FALSE)
)



models <- models %>%
  rep(length(unique(scotty_data$src_sub_area))) %>%
  enframe("model_name", "model") %>%
  mutate(src_sub_area =
      sort(rep(unique(scotty_data$src_sub_area), length(unique(.$model_name))))
  )

models

```

And finally, we can join the result into our nested data. Note that we could also apply some rule here. For example, if I don’t want to have ets() and auto.arima() for data with msts class–since they are not suitable for multiple seasonality time series–we can use filter to remove them out:

```{r}
# combine with models
scotty_data <- scotty_data %>%
  left_join(models) %>%
  filter(
    !(model_name == "ets" & data_fun_name == "msts"),
    !(model_name == "auto.arima" & data_fun_name == "msts")
  )

```

# EXECUTE THE NESTED FITTING

```{r}
scotty_ts_model <- scotty_data %>%
  mutate(
    params = map(train, ~ list(x = .x)),
    data = invoke_map(data_fun, params),
    params = map(data, ~ list(x = .x)),
    fitted = invoke_map(model, params)
  ) %>%
  select(-data, -params)

wd <-  as.character(getwd())
saveRDS(object=scotty_ts_model, file=paste(paste(wd,"/dscotty_ts_model/",sep = ""),"scotty_ts_model.rds",sep=""))

```


```{r}
scotty_ts_model <- saveRDS(data, "data_input/scotty_ts_model.rds")
  
scotty_ts_model
```


