---
title: "Demand Forecasting of Scotty Ride-sharing Service"
author: "by Gabriel Erichson"
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: true
    number_sections: true
    theme: sandstone
    highlight: haddock
    css:  style.css
  pdf_document: default
---

Reference:
[Forecasting: Principles and Practice](https://otexts.com/fpp2/)<br>
[Exploring & Visualizing Time Series](https://afit-r.github.io/ts_exploration)<br>

<head>
		<title>"Demand Forecasting</title>
		<link rel="icon" href="assets/logo.png" type="image/png">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
</head>

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	fig.align = "center",
	message = FALSE,
	warning = FALSE,
	comment = "#>",
	result = "hide"
)

options(scipen = 9999999)
#rm(list=ls())

#Data Wrangling and Visualization
library(Rcpp)
library(tidyverse)
library(ggplot2)
library(plotly)
library(scales)
library(lubridate)
library(tidyquant)
library(glue)
library(ggthemes)
library(paletti)
library(kableExtra)
library(sqldf)
library(gridExtra)
library(grid)
library(recipes)


#Time Series 
library(padr)
library(TSstudio)
library(forecast)
library(tseries)
library(magrittr)
library(yardstick)
library(timetk)

```


```{r, include=FALSE}
# WARNA
mycolorfill = c(
  
  light_blue="#2f4b7c", 
  smooth_blue ="#4B87CB",
  light_purple ="#665191",
  dark_pink="#a05195", 
  light_pink="#d45087", 
  light_red="#f95d6a", 
  dark_orange="#ff6347",
  semi_orange="#e79658",
  orange="#dda15a",
  cream="#b59378",
  dark_cream="#A57F5F",
  choc="#85664B",
  dark_choc="#6b5340",
  light_orange="#ff7c43"
)


viz_palette(mycolorfill)
mycolor_fill  <- get_scale_fill(get_pal(mycolorfill))
mycolor_color <- get_scale_color(get_pal(mycolorfill))
mycolor_hex <- get_hex(mycolorfill)

```


# Intro

Scotty Technologies Inc. ("Scotty") adalah sebuah perusahaan start-up teknologi yang didirikan pada tahun 2017 di Istanbul, Turkey. Salah satu layanan utama dari Scotty adalah *motorcycle ride-sharing* atau yang akrab kita ketahui *Ojek Online*.
Menurut informasi yang dilansir dari [markets.businessinsider.com](https://markets.businessinsider.com/news/stocks/bcap-invests-in-turkish-rideshare-company-scotty-1028819466), Scotty berencana untuk menjadi super-app pertama di Turkey dengan tekonologi dan model bisnis yang distruptif, serta berencana untuk melanjutkan pertumbuhan yang menjanjikan dan mencapai profitabilitas dalam waktu dekat. Berbicara mengenai pertumbuhan, tentu saja tidak luput dari demand. Oleh karena itu, projek ini ditujukan untuk membuat model analisa dan memprediksi demand transaksi per-jam pada layanan motorcycle ride-sharing di Scotty. Dalam proses pembuatan model ini, kita menggunakan data transaksi Scotty dari 2017-10-01 sampai 2017-12-02. <br>


# Problem Identification

Problem yang diselesaikan yaitu membuat model untuk memprediksi demand transaksi per-jam pada layanan motorcycle ride-sharing di Scotty. Namun yang hendak diprediksi dalam rentang waktu berapa lama? Mari kita cek dulu supaya dapat membantu proses pemodelan yang akan kita buat.

```{r}
# Read Data Test
scotty_actual_test <- read_csv("data_input/scotty_ts/data-test.csv")
glimpse(scotty_actual_test)
```

Struktut data diatas belum sesuai. Mari sesuaikan struktur datanya dan meilhat rentang waktu yang hendak diprediksi dahulu:
```{r}
scotty_actual_test <- scotty_actual_test %>% 
   mutate(
      # Transaksi Per-Jam
      datetime = floor_date(datetime, unit="hour"),
      demand = as.integer(demand)
   )
summary(scotty_actual_test)
```

Data diatas merupakan summary dari data yang hendak diprediksi. Berdasarkan informasi tersebut, dapat disimpullkan bawah masalah yang perlu kita selesaikan yaitu melakukan prediksi **demand per-jam** di sub area **sxk97, sxk9e dan sxk9** dalam rentang waktu dari **2017-12-03 00:00 sampai 2017-12-09 23:00** atau **24 jam** * **1 minggu**. Oke, mari kita mulai proses nya.


# Data Preparation
## Read Data
Berikut ini adalah data yang kita punya untuk melakukan analisa dan prediksi.

```{r}
scotty_input <- read_csv("data_input/scotty_ts/data-train.csv")
```

**5 Top Line Data**
```{r}
head(scotty_input)
```

<br>
**5 Bottom Line Data**
```{r}
tail(scotty_input,5)
```

***

<br>

## Variable Description

Variable              | Description
----------------------|------------------------------------------------------------
id                    | Transaction id
trip_id               | Trip id
driver_id             | Driver id
rider_id              | Rider id
start_time            | Transaction Time
src_lat               | Request source latitude
src_lon               | Request source longitude
src_area              | Request source area
src_sub_area          | Request source sub-area
dest_lat              | Requested destination latitude
dest_lon              | Requested destination longitude
dest_area             | Requested destination area
dest_sub_area         | Requested destination sub-area
distance              | Trip distance (in KM)
status                | Trip status (all status considered as a demand)
confirmed_time_sec    | Time different from request to confirmed (in seconds)

***

<br>

## Missing & Duplicate Value?
**Missing Value**
Jika dilihat terdapat missing value pada variabel **trip_id** dan **driver_id**, namun tidak menjadi masalah karena pada case ini kita butuh data waktu, sub area dan demand.
```{r}
colSums(is.na(scotty_input))
```
<br>

**Duplicate Value**
Dataset ini tidak memiliki data transaksi yang duplikat, sehingga bisa kita lanjutkan ke tahap pre-processing.
```{r}
data.frame(jumlah.observasi.awal=length(scotty_input$id),
           jumlah.observasi.unik=length(unique(scotty_input$id)))
```

***

<br>

# Data Pre-processing
## Data Structure
```{r}
glimpse(scotty_input)
```

Dataset yang dimiliki terdiri dari **90.133** observasi dan **16** variabel. Jika dilihat dari struktur datanya, beberapa variabel memiliki tipe data yang belum sesuai. Namun, karena projek ini bertujuan untuk melakukan time-series forecasting terhadap demand per-jam pada layanan motorcycle ride-sharing di Scotty, maka data yang kita perlukan adalah waktu transaksi per-jam dan demand pada setiap src_sub_area. Maka dari Berikut prosesnya:

```{r}
scotty_input <- scotty_input %>% 
   mutate(
      # Transaksi Per-Jam
      datetime = floor_date(start_time, unit="hour")
   ) %>% 
   group_by(src_sub_area, datetime) %>% 
   summarise(demand = length(status)) %>% 
   ungroup()
```


## Missing Sequential Datetime Value

Dalam melakukan analisa dan prediksi time series ada beberapa hal yang harus terpenuhi antara lain:<br>
  1. Data tidak boleh ada yang missing<br>
  2. Data harus terurut berdasarkan periode waktunya<br>
  3. Tidak boleh ada waktu atau periode yang terlewat/bolong<br>
  
Terkait case ini, Jam opearasional Scotty yaitu 24jam/hari, sehingga kita harus memastikan seluruh data jam sudah lengkap secara sekuensial dari rentang tanggal minimum hingga tanggal maximum. Jika terdapat deret waktu yang kosong, 
maka dapat dilakukan imputasi data dengan asumsi bahwa pada waktu tersebut memang tidak terdapat transaksi, sehingga **jumlah demand = 0**.

```{r, warning=FALSE, error=FALSE}
scotty_input <- scotty_input %>% 
  group_by(src_sub_area) %>% 
  # PAD tanggal sekuensial
  padr::pad(start_val = min(scotty_input$datetime), end_val = max(scotty_input$datetime)) %>% 
  ungroup() %>% 
  distinct() %>% 
  mutate(
    # replace NA Value menjadi 0
    demand = ifelse(is.na(demand),0,demand)
  ) %>% 
  arrange(datetime)
```

**Berikut data hari yang tidak memiliki transaksi:**
```{r}
scotty_input %>% filter(demand==0)
```

Data diatas menunjukan total waktu yang tidak terdapat transaksi dari sub Area **sxk97, sxk9e dan sxk9s** yaitu 301 Jam.



## Data Summary
```{r}
summary(scotty_input)
```

Informasi di atas merupakan summary keseluruhan data yang kita miliki. Dapat diketahui bahwa dataset ini merupakan data transaksi dari 3 sub area yaitu **sxk97, sxk9e dan sxk9s** yang terjadi dari **2017-10-01 00:00** sampai **2017-12-02 23:00** dengan **jumlah minimum 0 demand** dan **jumlah maximum 217 demand**. Demand 0 diasumsikan bahwa pada waktu tertentu memang tidak terdapat transaksi.


***

<br>

# Exploratory Data Analysis

## Demand Analysis by Sub-Area
Berikut ini visualisasi demand berdasarkan sub-area dari dari **2017-10-01 00:00** sampai **2017-12-02 23:00**. Titik/Poin berwarna merah, hijau dan biru menandakan data deret waktu tersebut kosong atau tidak terdapat transaksi pada waktu tersebut.

```{r, fig.width=8.5, fig.asp=0.75}
scotty_demand <- scotty_input 

scotty_demand <- scotty_demand %>% 
    mutate(
      weekdays = weekdays(datetime),
      popup = glue(
      "Category: {src_sub_area}
      Transaction Time: {datetime}
      Day: {weekdays}
      Demand: {demand}"
    )) 


plot_scotty_demand <- ggplot(scotty_demand,aes(x=datetime, y=demand),show.legend = FALSE)+
   #geom_point()+
  geom_line(show.legend = FALSE,size=0.3)+
  geom_point(aes(text=popup),size=0.05)+
  geom_point(data = scotty_demand %>% filter(demand==0),
             aes(x=datetime, y=demand, color=src_sub_area, text=popup),size=0.5, show.legend = FALSE)+
  labs(
    x = NULL, 
    y = NULL,
    title = "Demand by Sub-Area"
  ) +
  scale_x_datetime(date_breaks = "7 day", labels = date_format("%b %d"))+
  facet_wrap(~ src_sub_area, scale = "free", ncol = 1)+
  theme_minimal()+
  theme(
    legend.position = "none", 
    plot.title = element_text(hjust = 0.5),
    panel.spacing = unit(1, "lines"))

ggplotly(p = plot_scotty_demand, tooltip="text") %>% 
  layout(showlegend=FALSE)
```

Pada grafik diatas dapat dilihat bahwa jumlah demand dari ketiga sub-area relatif sama dan juga terdapat **pola seasonal** meskipun belum terlihat jelas detailnya. Selain itu, juga dapat dilihat demand pada **Jumat,03 November 2017 pukul 18.00** dan **Jumat, 24 November 2017 pukul 18.00** jauh lebih tinggi dari hari lainnya. Sayangnya data yang kita miliki belum dapat menjawab hal tersebut, namun mari kita fokus ke demand. Berikut adalah demand per-harinya: 

```{r,fig.width=12.5}
# Daily Demand
polar_day <- scotty_demand %>% 
  mutate(day=weekdays(datetime)) %>% 
  group_by(src_sub_area,day) %>% 
  summarise(demand_per_day=sum(demand)) %>% 
  ungroup() %>% 
  mutate(
    day = ordered(day, levels=c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"))
  )

serror <- function(x) sqrt(var(x)/length(x)) 

ggplot(polar_day %>% filter(src_sub_area=="sxk97"), aes(x=day, y=demand_per_day, fill = day)) +
  geom_bar(width = 1, stat = "identity", color = "white", show.legend = FALSE) +
  geom_errorbar(aes(ymin = demand_per_day - serror(demand_per_day), 
                    ymax = demand_per_day + serror(demand_per_day), 
                    color = day), 
                    width = .2) + 
  scale_y_continuous(breaks = 0:nlevels(day)) +
  labs(
    title = "Daily Demand in Sub-Area sxk97"
  )+
 theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, size=12, face="bold"),
        axis.text.y = element_blank(),
        axis.text.x=element_text(face="bold"))+
  coord_polar() -> day_polar1

ggplot(polar_day %>% filter(src_sub_area=="sxk9e"), aes(x=day, y=demand_per_day, fill = day)) +
  geom_bar(width = 1, stat = "identity", color = "white", show.legend = FALSE) +
  geom_errorbar(aes(ymin = demand_per_day - serror(demand_per_day), 
                    ymax = demand_per_day + serror(demand_per_day), 
                    color = day), 
                    width = .2) + 
  scale_y_continuous(breaks = 0:nlevels(day)) +
  labs(
    title = "Daily Demand in Sub-Area sxk9e"
  )+
  theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
         plot.title = element_text(hjust = 0.5, size=12, face="bold"),
        axis.text.y = element_blank(),
        axis.text.x=element_text(face="bold"))+
  coord_polar() ->day_polar2


ggplot(polar_day %>% filter(src_sub_area=="sxk9s"), aes(x=day, y=demand_per_day, fill = day)) +
  geom_bar(width = 1, stat = "identity", color = "white", show.legend = FALSE) +
  geom_errorbar(aes(ymin = demand_per_day - serror(demand_per_day), 
                    ymax = demand_per_day + serror(demand_per_day), 
                    color = day), 
                    width = .2) + 
  scale_y_continuous(breaks = 0:nlevels(day)) +
  labs(
    title = "Daily Demand in Sub-Area sxk9s"
  )+
 theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,size=12, face="bold"),
        axis.text.y = element_blank(),
        axis.text.x=element_text(face="bold"))+
  coord_polar() ->day_polar3


grid.arrange(day_polar1,day_polar2,day_polar3, ncol = 3)

```


Grafik diatas memberikan informasi total demmand per-hari dari masing-masing Sub-Area. Secara keseluruhan, ketiga Sub-Area menunjukan pola demand per-hari yang sama, dimana **total demand paling besar yaitu pada hari Jumat** dan **total demand paling kecil yaitu pada hari Minggu**. Mari kita lihat data demand per-jamnya berikut:

```{r, fig.width=12}
# Hourly Demand
polar_hourly <- scotty_demand %>% 
  mutate(hour=hour(datetime)) %>% 
  group_by(src_sub_area,hour) %>% 
  summarise(demand_per_hour=sum(demand)) %>% 
  ungroup() %>% 
  mutate(
    hour = ordered(hour,levels=c("0","1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17",
                              "18","19","20","21","22","23"))
  )

ggplot(polar_hourly %>% filter(src_sub_area=="sxk97"), aes(x=hour, y=demand_per_hour, fill = hour)) +
  geom_bar(width = 1, stat = "identity", color = "white", show.legend = FALSE) +
  geom_errorbar(aes(ymin = demand_per_hour - serror(demand_per_hour), 
                    ymax = demand_per_hour + serror(demand_per_hour), 
                    color = hour), 
                    width = .2) + 
  scale_y_continuous(breaks = 0:nlevels(hour)) +
  labs(
    title = "Hourly Demand in Sub-Area sxk97",
    subtitle = "24 Hour Format"
  )+
 theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,size=12, face="bold"),
        plot.subtitle = element_text(hjust = 0.5,size=11),
        axis.text.y = element_blank(),
        axis.text.x=element_text(size=11, face="bold"))+
  coord_polar() -> hour_polar1


ggplot(polar_hourly %>% filter(src_sub_area=="sxk9e"), aes(x=hour, y=demand_per_hour, fill = hour)) +
  geom_bar(width = 1, stat = "identity", color = "white", show.legend = FALSE) +
  geom_errorbar(aes(ymin = demand_per_hour - serror(demand_per_hour), 
                    ymax = demand_per_hour + serror(demand_per_hour), 
                    color = hour), 
                    width = .2) + 
  scale_y_continuous(breaks = 0:nlevels(hour)) +
  labs(
    title = "Hourly Demand in Sub-Area sxk9e",
    subtitle = "24 Hour Format"
  )+
 theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,size=12, face="bold"),
        plot.subtitle = element_text(hjust = 0.5,size=11),
        axis.text.y = element_blank(),
        axis.text.x=element_text(size=11, face="bold"))+
  coord_polar() -> hour_polar2


ggplot(polar_hourly %>% filter(src_sub_area=="sxk9s"), aes(x=hour, y=demand_per_hour, fill = hour)) +
  geom_bar(width = 1, stat = "identity", color = "white", show.legend = FALSE) +
  geom_errorbar(aes(ymin = demand_per_hour - serror(demand_per_hour), 
                    ymax = demand_per_hour + serror(demand_per_hour), 
                    color = hour), 
                    width = .2) + 
  scale_y_continuous(breaks = 0:nlevels(hour)) +
  labs(
    title = "Hourly Demand in Sub-Area sxk9s",
    subtitle = "24 Hour Format"
  )+
 theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,size=12, face="bold"),
        plot.subtitle = element_text(hjust = 0.5,size=11),
        axis.text.y = element_blank(),
        axis.text.x=element_text(size=11, face="bold"))+
  coord_polar() -> hour_polar3


grid.arrange(hour_polar1,hour_polar2,hour_polar3, ncol = 3)

```

Grafik diatas memberikan informasi total demmand per-jam dari masing-masing Sub-Area. Secara keseluruhan, ketiga Sub-Area menunjukan pola demand per-jam yang sama, dimana **total demand relatif tinggi dari pukul 17:00 hingga pukul 19:00** dan **total demmand relatif rendah dari pukul 01:00 hingga pukul 05:00**. Namun, yang berbeda adalah **total demand sub-area sxk9e juga cukup tinggi pada pukul 08:00**, hal ini berbeda dengan sub-area lainnya. 


***

<br>

## Seasonal Analysis

Sebelum melakukan forecasting kita perlu mengetahui informasi tren dan seasonal yang ada pada data. Salah satu cara untuk mengetahuinya yaitu melakukan Decompose. Decompose merupakan tahapan dalam analisis time series yang digunakan untuk menguraikan beberapa komponen yang berupa:<br>

  1. *Data* : data objek time-series yang diamati.<br>
  2. *Trend* : pola data secara general, cenderung untuk naik atau turun.<br>
  3. *Seasonal* : pola musiman yang membentuk pola berulang pada periode waktu yang tetap.<br>
  4. *Residual/Error* : pola yang tidak dapat ditangkap dalam trend dan seasonal.<br>
  
Jika pada hasil decompose, trend masih membentuk sebuah pola maka dapat dicurigai masih ada seasonality yang belum ditangkap oleh frekuensi data. Seharusnya trend cenderung naik atau cendurung turun atau membentuk garis yang smooth. 

Sesuai dengan masalah yang hendak diselesaikan yaitu melakukan prediksi demand per-jam pada masing-masing sub-area selama 1 minggu, maka mari kita coba decompose secara harian dan mingguan terhadap salah satu sub-area.


```{r, fig.width=8}
sxk9s <- scotty_input %>% filter(src_sub_area == "sxk9s") %>% select(-src_sub_area)

sxk9s_ts_weekly <- sxk9s %>% .$demand %>% ts(frequency = 24*7)
  
plot_sxk9s_ts_weekly <- sxk9s_ts_weekly %>% 
  tail(24*7*4) %>% 
  decompose() %>% 
   autoplot()+ 
    theme_minimal()+
    theme(
          title = element_text(size=11),
          axis.title=element_text(size=9, face="bold"),
          axis.text.x=element_text(size=9),
          axis.text.y=element_text(size=9),
          legend.position = "none",
          panel.spacing = unit(4, "pt")
          )


ggplotly(plot_sxk9s_ts_weekly) %>% 
  layout(title = list(text = paste0('Decomposition on Weekly in Sub-Area sxk9s',
                                    '<br>',
                                    '<sup>',
                                    'Deompose single seasonal time-series object using frequency = 24*7 (Weekly)',
                                    '</sup>')))

```

Hasil decompose objek single sesonal time-series diatas menggunakan frekuensi Mingguan pada data 1 bulan terkahir. Beberapa informasi yang bisa kita dapat yaitu:<br>

  1. Panel *data* merupakan data observasi time-series scotty pada data 1 bulan terkahir. Data ini sebenarnya sama seperti yang ditampilkan pada poin 5.1 Demand Analysis, namun karena pola kurang terlihat pada poin 5.1 tersebut maka kita perkecil analisanya menggunakan data 1 bulan terakhir.<br>
  2. Panel *seasonal* merupakan komponen seasonal. Jika dilihat terdapat 4 bagian grafik yang memuncak paling tinggi dan yang lainnya rendah. Berhubung data yang kita time-series yang kita buat menggunakan frekuensi mingguan, maka hal ini dapat mengindikasikan bahwa pola seasonal yang terjadi sebenarnya secara harian dan mingguan.<br>
  3. Panel *trend* merupakan komponen trend. Pola trend yang ditampilkan masih belum smooth dan terkesan masih membentuk pola pada beberapa bagian, mungkin saja terdapat pola seasonal yang belum ditangkap. Hal ini masuk akal dengan yang ditampilkan pada panel seasonal, sehingga mengindikasikan bahwa data kita merupakan *multiple seasonal time series object*.<br> 
  4. Panel *Remainder/Error* merupakan komponen error yang menunjukan pola yang tidak dapat ditangkap dalam trend dan seasonal.<br>


Hasil decompose diatas mengindikasikan bahwa data time-series yang kita observasi memiliki pola *Multi-Seasonal*. Berdasarkan pola seasonalnya menunjukan bahwa data ini memiliki seasonal harian dan mingguan, sehingga mari kita mebuat data multiple seasonal time series dan melakukan decompose kembali.


```{r, fig.width=8}
sxk9s_msts_weekly <- sxk9s %>% .$demand %>%  
  msts(seasonal.periods = c(24, 24*7))

plot_sxk9s_msts_weekly <- sxk9s_msts_weekly %>% 
  tail(24*7*4) %>% 
   mstl() %>% 
   autoplot()+ 
   labs(
      title = "Decomposition on Daily and Weekly in Sub-Area sxk9s"
    )+
    theme_minimal()+
    theme(
          title = element_text(size=12),
          axis.title=element_text(size=9, face="bold"),
          axis.text.x=element_text(size=9),
          axis.text.y=element_text(size=9),
          legend.position = "none",
          panel.spacing = unit(4, "pt")
          ) 

ggplotly(plot_sxk9s_msts_weekly) %>% 
    layout(title = list(text = paste0('Decomposition on Daily & Weekly in Sub-Area sxk9s',
                                    '<br>',
                                    '<sup>',
                                    'Deompose multiple seasonal time-series object using seasonal.periods = c(24, 24*7) (Daily & Weekly)',
                                    '</sup>')))

```

Pada hasil decompose objek multiple seasonal time-series diatas, dapat dilihat pola tren terlihat lebih smooth dan tidak membentuk pola. Hal ini mengindikasikan bahwa kemungkinan besar seluruhan seasonal sudah ditangkap. Untuk lebih jelasnya, berikut ini visualisasi seasonal perjam-nya dalam harian dan mingguan pada data observasi kita:


```{r, fig.width=8}
sxk9s_msts_weekly_dc <- mstl(sxk9s_msts_weekly %>% tail(24*7*4))

plot_seasonal_msts <- as.data.frame(sxk9s_msts_weekly_dc) %>% 
  mutate(
    datetime = sxk9s %>% tail(24*7*4) %>% .$datetime
  ) %>% 
  mutate(
    day = wday(datetime, label = TRUE, abbr = FALSE),
    hour = as.factor(hour(datetime))
  ) %>% 
  group_by(day, hour) %>% 
  summarise(
    seasonal = sum(Seasonal24 + Seasonal168)
  ) %>% 
  ggplot(mapping = aes(x = hour, y = seasonal)) +
  geom_col(aes(fill = day)) +
  scale_fill_viridis_d(option = "plasma") +
  labs(
    title = "Seasonal Analysis based Daily and Weekly in Sub-Area sxk9s",
    fill = "Day of Week"
  ) + 
  theme_minimal() +
  theme(
        title = element_text(size=11),
        axis.title=element_text(size=9, face="bold"),
        axis.text.x=element_text(size=9),
        axis.text.y=element_text(size=9),
        legend.position = "right"
        ) 

ggplotly(plot_seasonal_msts)

```


# Cross Validation
Dalam case ini kita akan mencoba membuat beberapa model, tentunya kita akan membagi seluruh data yang kita punya menjadi data train dan data test. Problem yang diselesaikan yaitu memprediksi demand perjam dalam 1 minggu sehingga saya akan membagi data test menggunakan data 1 minggu terakhir dan sisanya sebagai data train. Berikut interval waktu di data train dan data test:

```{r}
scotty_data <- scotty_input

#data test 1 minggu
test_size <- 24*7

test_end <- max(scotty_data$datetime)
test_start <- test_end - hours(test_size) + hours(1)

train_end <- test_start - hours(1)
train_start <- min(scotty_data$datetime)

# get the interval of each samples
intrain <- interval(train_start, train_end)
intest <- interval(test_start, test_end)

data.frame("interval_data_train"=intrain,
              "interval_data_test"=intest)

scotty_data <- scotty_data %>% 
  mutate(sample = case_when(
        datetime %within% intrain ~ "train",
        datetime %within% intest ~ "test")) %>% 
  drop_na() %>% 
  mutate(sample = factor(sample, levels = c("train", "test")))
  
#head(scotty_data)

```

**Berikut ini visualisasi pembagian data train dan data test dalam deret waktu:**

```{r, fig.width=8}
ggplotly(
 scotty_data %>% 
  ggplot(aes(x = datetime, y = demand, colour = sample)) +
    geom_line() +
    labs(
      x = NULL, 
      y = NULL,
      title = "Demand by Sub-Area on Data Train Vs Data Test"
    ) +
    scale_x_datetime(date_breaks = "7 day", labels = date_format("%b %d"))+
    facet_wrap(~ src_sub_area, scale = "free", ncol = 1) +
    theme_minimal()+
    theme(
      legend.position = "bottom", 
      plot.title = element_text(hjust = 0.5),
      panel.spacing = unit(1, "lines"))+
    mycolor_color()
  ) %>% 
  layout(
    xaxis = list(showticklabels = FALSE),
    legend=list(orientation = "h",
                   y = -0.1, x = 0.4)
  )

```


Dapat dilihat pada visualisasi di atas, dimana jumlah demand di 3 Sub-Area berbeda-beda dan pada waktu-waktu tertentu memiliki selisih jauh berbeda. Hal ini dapat mengindikasikan outlier, sehingga perlu dilakukan **scaling data train pada masing-masing Sub-Area** sebelum memulai pemodelan supaya tidak sensitif pada data outlier. Berikut prosesnya:

**1. Convert data to wide format**
```{r}
# Spread data menjadi 3 Sub-Area 
scotty_data <- scotty_data %>%
  spread(src_sub_area, demand)

head(scotty_data)

```

**2. Scalling data**
```{r}
# recipes: square root, center, scale
scalling <- recipe(~ ., filter(scotty_data, datetime %within% intrain)) %>%
  step_sqrt(all_numeric()) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  prep()

# preview the bake results
scotty_data <- bake(scalling, scotty_data)

head(scotty_data)
```


Data yang sudah di-scalling tentunya nanti perlu dikembalikan lagi menjadi nilai aslinya, maka dibuat fungsi **revert_back**:

```{r}
revert_back <- function(vector, recipe, varname) {
  # store recipe values
  rec_center <- recipe$steps[[2]]$means[varname]
  rec_scale <- recipe$steps[[3]]$sds[varname]
  
  # convert back based on the recipe
  results <- (vector * rec_scale + rec_center) ^ 2
  
  # add additional adjustment if necessary
  results <- round(results)
  
  # return the results
  results
}

```



**3. Convert data to long format**<br>
Berikut ini merupakan struktur data kita setelah dilakukan cross validation train-test dan scalling:

variabel **datetime** merupakan data waktu transaksi.
variabel **sample** merupakan data tipe factor yang terbagi **train** dan **test**, sehingga kita dapat mengidentifikasi data yang hendak digunakan dalam pemodelan dan evaluasi model.
variabel **src_sub_area** merupakan data nama dari Sub-Area.
variabel **demand** merupakan data demand.

```{r}
scotty_data <- scotty_data %>% 
  gather(src_sub_area, demand, -datetime, -sample)

head(scotty_data)
```


# Nested Modelling

Pada pemodelan ini kita akan menggunakan fungsi `purr` dan untuk menggunakan fungsi `purr`, maka kita perlu merubah dataset kita yang sebelumnya berbentuk *table* menjadi *nested table*, berikut lebih jelasnya:

```{r}
scotty_data <- scotty_data %>%
  group_by(src_sub_area, sample) %>%
  nest(.key = "data") %>%
  spread(sample, data)

scotty_data
```

Data diatas merupakan format nested table, variabel **src_sub_area** merupakan nama sub-area, variabel **train** merupakan bentuk nested table yang isinya merupakan data **datetime dan demand untuk data train**, dan variabel **test** juga merupakan bentuk nested table yang isinya merupakan data **datetime dan demand untuk data test**.


## Data Time-Series Object List

Pada bagian Seasonal Analysis, kita sudah melihat bahwa data kita merupakan Mutiple Seasonal sehingga data kita akan lebih cocok apabila dikonversi keformat **msts** dengan seasonal harian dan mingguan, namun dengan menggunakan konsep nested table kita juga dapat mengecek dengan mudah apabila data kita dikonversi keformat **ts**. Sehingga, kita akan mencoba melakukan pemodelan menggunakan **single seasonal time-series dengan frequency harian** dan **multiple seasonal time-series dengan seasonal harian dan mingguan** pada masing-masing Sub-Area.

```{r}
# Membuat list model data time-series
data_model <- list(
  # single seasonal time-series dengan frequency harian
  ts = function(x) ts(x$demand, frequency = 24), 
  
  # multiple seasonal time-series dengan seasonal harian dan mingguan
  msts = function(x) msts(x$demand, seasonal.periods = c(24, 24*7))
)

# convert to nested table
data_model <- data_model %>%
  rep(
    length(unique(scotty_data$src_sub_area))
  ) %>%
  enframe("data_fun_name", "data_fun") %>%
  mutate(src_sub_area =
    sort(
      rep(unique(scotty_data$src_sub_area), length(unique(.$data_fun_name)))
         )
  )

data_model

```
 
Data diatas menunjukan data time-series pada masing-masing sub-area. mari kita satukan dengan nested table sebelumnya untuk mempermudah pemodelan.

```{r}
# join time-series object to data
scotty_data <- scotty_data %>%
  left_join(data_model)

scotty_data
```

Data diatas merupakan nested table antara data train dan data tes pada masing-masing sub-area yang digabungkan dengan objek time series yang sudah dibuat. Sampai dengan proses ini, berarti kita sudah memiliki objek **single seasonal time-series dengan frequency harian** dan **multiple seasonal time-series dengan seasonal harian dan mingguan** pada masing-masing Sub-Area. Mari kita siapkan model-model untuk masing-masing objek.<br>


## Time Series Model List

Untuk case ini kita akan mencoba menggunakan beberapa model, antara lain:<br>

  1. ETS <br>
  2. Holt Winter Exponential Smoothing <br>
  3. Seasonal ARIMA <br>
  4. STLM method="ets" <br>
  5. STLM method="arima" <br>
  6. TBATS <br>

Berdasarkan proses decompose, kita ketahui bahwa objek time-series ini memiliki trend dan seasonal. Sehingga seharusnya hanya metode yang bisa mengatasi efek trend dan seasonal yang kita gunakan, namun mari kita coba seluruh model diatas terhadap data **single seasonal** dan **multiple seasonal** yang sudah kita buat. Kemudian, untuk menggenerate seluruh modelnya, maka seluruh model akan kita join/merge dengan nested table dari objek time-series yang sudah kita buat sebelumnya. Berikut proses dan hasilnya:

```{r}
#models list
models <- list(
  ets = function(x) ets(x),
  #Model Holt’s Winter Exponential Smoothing
  holt_winters = function(x) HoltWinters(x),
  # Seasonal ARIMA (SARIMA)
  auto_arima = function(x) {
    while (adf.test(x)$p.value > 0.05) {
       x = diff(x)
     }
    auto.arima(x, seasonal = TRUE)
  },
  #STLM ets 
  stlm_ets = function(x) stlm(x, method="ets"),
  #STLM arima
  stlm_arima = function(x) stlm(x, method="arima"),
  
  #TBATS
  tbats = function(x) tbats(x, use.box.cox = FALSE, 
                  use.trend = TRUE, 
                  use.damped.trend = TRUE,
                  use.parallel = FALSE)
)

#Nested Model by sub-area
models <- models %>%
  rep(length(unique(scotty_data$src_sub_area))) %>%
  enframe("model_name", "model") %>%
  mutate(src_sub_area =
      sort(rep(unique(scotty_data$src_sub_area), length(unique(.$model_name))))
  )


#join time-series object and model
scotty_models <- scotty_data %>%
  left_join(models) 
  # filter(
  #   !(model_name == "ets" & data_fun_name == "msts"),
  #   !(model_name == "auto_arima" & data_fun_name == "msts"),
  #   !(model_name == "stlm_ets" & data_fun_name == "msts"),
  #   !(model_name == "stlm_arima" & data_fun_name == "msts")
  # )

scotty_models

```


## Execute Nested Model Fitting

Chunk di bawah ini digunakan untuk menggenerate seluruh model yang dibuat. Chunk ini di-set **eval=FALSE** karena cukup memakan waktu. Hasil generate model disimpan dengan format file .rds.
```{r, eval=FALSE}
scotty_ts_model <- scotty_models %>%
  mutate(
    params = map(train, ~ list(x = .x)),
    data = invoke_map(data_fun, params),
    params = map(data, ~ list(x = .x)),
    fitted = invoke_map(model, params)
  ) %>%
  select(-data, -params)

models <- scotty_ts_model
wd <-  as.character(getwd())
saveRDS(object=models, file=paste(paste(wd,"/scotty_ts_model/",sep = ""),"scotty_ts_model.rds",sep=""))

```

Mari kita breakdown dulu. Nested table yang sebelumnya kita buat terdiri dari 3 Sub-Area, dan setiap sub-area dibuat menjadi 2 objek time series yaitu **single seasonal** dan **multiple seasonal**, kemudian kita membuat 6 model yang akan digunakan berdasarkan sub-area dan jenis objek time-series nya. Sehingga, totalnya kita sudah membuat 36 model time-series. Berikut ini hasil generate seluruh model yang sudah dibuat dan siap untuk digunakan.

```{r}
scotty_ts_model <- readRDS("scotty_ts_model/scotty_ts_model.rds")
head(scotty_ts_model)
```


## Model Evaluation

Berikut ini adalah hasil evaluasi seluruh model berdasarkan nilai **Mean Absolute Error (MAE)**. Jika dilihat secara manual, **nilai MAE paling kecil** pada setiap sub-area dihasilkan oleh model **TBATS** menggunakan objek **Multiple Seasonal Time Series**. Berikut ini data MAE jika kita melakukan forecasting terhadap data train dan juga data test yang sudah kita siapkan.

```{r}
scotty_model_eval <- scotty_ts_model %>% 
  mutate(
    #evaluasi model terhadap data train
    mae_train = map(fitted, ~ forecast(.x, h = test_size)) %>%
                      map2_dbl(train, ~ mae_vec(truth = revert_back(.y$demand,scalling,src_sub_area), 
                                                estimate = revert_back(.x$fitted,scalling,src_sub_area))),
    #evaluasi model terhadap data test
     mae_test = map(fitted, ~ forecast(.x, h = test_size)) %>%
                      map2_dbl(test, ~ mae_vec(truth = revert_back(.y$demand,scalling,src_sub_area), 
                                               estimate = revert_back(.x$mean,scalling,src_sub_area)))
    ) %>% 
  arrange(src_sub_area, mae_test, mae_train) 




scotty_model_eval %>% select(src_sub_area,data_fun_name, model_name,fitted,mae_train,mae_test)

```

## Forecasting on Data Test

Berikut ini adalah hasil forecasting dari seluruh model terhadap data test.
```{r}
scotty_forecast <- scotty_model_eval %>%
  mutate(
    forecast =
      map(fitted, ~ forecast(.x, h = test_size)) %>%
      map2(test, ~ tibble(
        datetime = .y$datetime,
        demand = as.vector(.x$mean)
      )),
    key = paste(data_fun_name, model_name, sep = "-")
  )

# Extract Value using unnest
scotty_forecast <- scotty_forecast %>% 
  select(src_sub_area, key, actual = test, forecast) %>%
  spread(key, forecast) %>%
  gather(key, value, -src_sub_area) %>% 
  unnest(value) %>% 
  mutate(demand = revert_back(demand,scalling,src_sub_area)) %>% 
  mutate(
    popup = glue(
      "Data Model: {key}
      Sub Area: {src_sub_area}
      Datetime: {datetime}
      Demand: {demand}"
    )
  ) 
  
scotty_forecast %>%
 select(-popup) %>% 
 spread(key,demand) 
```

<br>

**Untuk lebih jelasnya, berikut visualisasi terhadap hasil forecasting diatas:**

```{r, fig.width=8}

plot_scotty_forecast <- scotty_forecast %>% 
  ggplot(aes(x = datetime, y = demand)) +
    #geom_line(data = scotty_input,aes(y = demand),alpha = 0.2,size = 0.8)+
    geom_line(data = scotty_forecast %>% filter(key == "actual"),aes(y = demand),alpha = 0.2,size = 0.8)+
    geom_point(data = scotty_forecast %>% filter(key == "actual"),aes(y = demand, text=popup),alpha = 0.1,size=0.1)+
    geom_line(data = scotty_forecast %>% filter(key != "actual"),aes(frame = key,col = key)) +
    geom_point(data = scotty_forecast %>% filter(key != "actual"),
               aes(frame = key,col = key, text=popup),alpha = 0.1, size=0.1)+
    geom_point(data = scotty_forecast %>% filter(key != "actual",demand==0),
               aes(frame = key,col = "black", text=popup),alpa=0.5, size=1)+
    labs(
      x = "Date", 
      y = "Demand",
      title = "Comparison of Time Series Forecasting Models Result on Data Test", 
      frame = "Models"
    ) +
    scale_x_datetime(date_breaks = "1 day", labels = date_format("%b %d"))+
    facet_wrap(~ src_sub_area, scale = "free", ncol = 1) +
    # scale_color_manual(values = c(color,color,color,color,color,color,color,color,color)) +
    theme_minimal()+
    theme(legend.position = "none", 
      plot.title = element_text(hjust = 0.5),
      panel.spacing = unit(1, "lines"))+
    mycolor_color()

ggplotly(plot_scotty_forecast, tooltip = "text", height = 560) %>% 
  animation_opts(
    2000, eas = "elastic", redraw = TRUE
  ) %>% 
  animation_slider(
    currentvalue = list(prefix = "Model: ", font = list(color=mycolor_hex("choc")))
  )

```

<br>

## Automated Best Model Selection

Berikut ini adalah model terbaik yang dapat diterapkan pada ketiga sub-area. Error paling kecil pada ketiga sub-area dihasilkan oleh model **TBATS** dengan objek **Multiple Seasonal Time Series** dengan nilai **MAE < 10**.
 
```{r}
# Minimum error for each sub-area
scotty_best_model <- scotty_model_eval %>%
  select(-fitted) %>% # remove unused
  group_by(src_sub_area) %>%
  filter(mae_test == min(mae_test)) %>%
  ungroup()

scotty_best_model
```

Sebelumnya kita menggunakan data train dan melakukan splitting menjadi tranning set dan test set untuk menentukan model. Untuk memprediksi data aktual maka kita harus menggabungkannya kembali seluruh datanya supaya deret waktu lengkap.

```{r}
# Menggabungkan data test dan train
scotty_final_model <- scotty_best_model %>%
  mutate(
    fulldata = map2(train, test, ~ bind_rows(.x, .y))) %>%
  select(src_sub_area, fulldata, everything(), -train, -test)

```

Chunk di bawah ini digunakan untuk menggenerate **final model** yang dibuat. Chunk ini di-set **eval=FALSE** karena cukup memakan waktu. Hasil generate model disimpan dengan format file .rds.

```{r, eval=FALSE}
#Execute Nested Final Model
scotty_final_model <- scotty_final_model %>%
  mutate(
    params = map(fulldata, ~ list(x = .x)),
    data = invoke_map(data_fun, params),
    params = map(data, ~ list(x = .x)),
    fitted = invoke_map(model, params)
  ) %>%
  select(-data, -params)

final_models <- scotty_final_model
wd <-  as.character(getwd())
saveRDS(object=final_models, file=paste(paste(wd,"/scotty_ts_model/",sep = ""),"scotty_final_model.rds",sep=""))
```

***

<br>


# Forecasting Result

Berdasarkan problem yang ingin diselesaikan maka kita perlu akan melakukan prediksi **demand per-jam** di sub area **sxk97, sxk9e dan sxk9** dalam rentang waktu **1 minggu kedepan** atau dari **2017-12-03 00:00 sampai 2017-12-09 23:00**. Berikut ini hasil forecastingnya:

```{r}
# Read Model
scotty_final_model <- readRDS("scotty_ts_model/scotty_final_model.rds")

# Forecasting 
time_size <- 24*7

next_week_prediction <- scotty_final_model %>% 
  mutate(forecast =
    map(fitted, ~ forecast(.x, h = time_size)) %>%
    map2(fulldata, ~ tibble(
      datetime = timetk::tk_make_future_timeseries(.y$datetime, time_size),
      demand = as.vector(.x$mean)
    ))
  )


# Extract Forecasting Result using unnest
next_week_prediction_result <- next_week_prediction %>% 
  select(src_sub_area, actual = fulldata, forecast) %>%
  gather(key, value, -src_sub_area) %>% 
  unnest(value) %>% 
  mutate(demand = revert_back(demand,scalling,src_sub_area)) 


# Display Forecasting Result  
next_week_prediction_result %>%
 filter(key=="forecast") %>% 
 spread(src_sub_area,demand) 


```

<br>

**Berikut visualisasi forecast dari 2017-12-03 00:00 sampai 2017-12-09 23:00**

```{r, fig.width=8}

vis_forecast <- next_week_prediction_result %>% 
  filter(month(datetime)>=11) %>% 
  mutate(
    popup = glue(
      "Data Model: {key}
      Sub Area: {src_sub_area}
      Datetime: {datetime}
      Demand: {demand}"
    )
  ) 

plot_viz_forecast <- vis_forecast %>% 
  ggplot(aes(x = datetime, y = demand, colour = key)) +
    geom_line() +
   geom_point(aes(text=popup),size=0.05)+
   geom_point(data = vis_forecast %>% filter(demand==0),
             aes(x=datetime, y=demand, text=popup),size=0.5, alpha=0.5, show.legend = FALSE)+
    labs(
      title = "Forecasting Result in Sub-Area sxk97, sxk9e and sxk9"
    ) +
    scale_x_datetime(date_breaks = "7 day", labels = date_format("%b %d"))+
    facet_wrap(~ src_sub_area, scale = "free", ncol = 1) +
    theme_minimal()+
    theme(
      legend.position = "bottom", 
      plot.title = element_text(hjust = 0.5),
      panel.spacing = unit(1, "lines"))+
    mycolor_color()


ggplotly(
 plot_viz_forecast, tooltip = "text", height=560
  ) %>% 
  layout(
    legend=list(orientation = "h",
                   y = -0.1, x = 0.4)
  )%>% 
  layout(title = list(text = paste0('Hourly Demand Forecasting Result in Sub-Area sxk97, sxk9e and sxk9',
                                    '<br>',
                                    '<sup>',
                                    'Forecasting result from December 3,2017 00:00 to December 09, 2017 23:00',
                                    '</sup>')))

  

```


